[
  {
    "objectID": "teaching.html",
    "href": "teaching.html",
    "title": "Teaching",
    "section": "",
    "text": "Autumn 2019: Probability and statistics – a series of lectures on selected topics of “Probability and Statistics” for Master/Ph.D. Students and researchers from the Pharmacometrics research group at Uppsala University.\nSpring 2019: Innovative clinical trials – Connecting theory and practice through adaptive designs and digital development – a Master/Ph.D. level course taught at the Center for Interdisciplinary Mathematics – CIM, Uppsala University.\nSpring 2018: Optimal designs and randomization techniques for clinical trials – a Master/Ph.D. level course taught at the Center for Interdisciplinary Mathematics – CIM, Uppsala University."
  },
  {
    "objectID": "posts/20200229/pythagorean-theorem-day.html",
    "href": "posts/20200229/pythagorean-theorem-day.html",
    "title": "Pythagorean theorem day",
    "section": "",
    "text": "In mathematics, the Pythagorean theorem gives a relation in Euclidean geometry among the three sides of a right triangle. It states that the area of the square which side is the hypotenuse (the side opposite the right angle) is equal to the sum of the areas of the squares on the other two sides.\nIt can be written in the form of the following equation:\n\\[\n  a^2+b^2 = c^2.\n\\tag{1}\\]\nFigure 1 shows a geometrical interpretation of the Pythagorean theorem.\n\n\n\n\n\nFigure 1: Geometry of the Pythagorean theorem"
  },
  {
    "objectID": "posts/20200229/pythagorean-theorem-day.html#pythagorean-theorem-day",
    "href": "posts/20200229/pythagorean-theorem-day.html#pythagorean-theorem-day",
    "title": "Pythagorean theorem day",
    "section": "Pythagorean theorem day",
    "text": "Pythagorean theorem day\nI found a funny definition of what is a Pythagorean theorem day. It is the day on which date in the format “yy-mm-dd” satisfies the following equality:\n\\[\n\\text{yy}^2 = \\text{mm}^2 + \\text{dd}^2.\n\\]\nFor example, August 15, 2017 can be written as 17-08-15, and it is a Pythagorean theorem day since\n\\[\n17^2 = 8^2 + 15^2.\n\\] I was curious how many days in the 21st century are the Pythagorean theorem days?"
  },
  {
    "objectID": "posts/20200229/pythagorean-theorem-day.html#r-code-to-find-the-pythagorean-theorem-days",
    "href": "posts/20200229/pythagorean-theorem-day.html#r-code-to-find-the-pythagorean-theorem-days",
    "title": "Pythagorean theorem day",
    "section": "R code to find the Pythagorean theorem days",
    "text": "R code to find the Pythagorean theorem days\n\nlibrary(lubridate)\n\npythagorean_days <- tidyr::crossing(\n  yy = seq(0, 99),\n  mm = seq(1, 12),\n  dd = seq(1, 31)\n) %>% \n  dplyr::mutate(\n    year = as.character(2000+yy),\n    month = stringr::str_pad(mm, width = 2, pad = \"0\"),\n    day = stringr::str_pad(dd, width = 2, pad = \"0\")\n  ) %>% \n  dplyr::mutate(\n    # making a character representation of a date\n    date_txt = stringr::str_glue('{year}{month}{day}'),\n    # converting character representation of a date to a Date object.\n    # if it is an inappropriate date, the function returns NA.\n    date = lubridate::ymd(\n      date_txt, \n      quiet = TRUE\n    )\n  ) %>% \n  # removing row with inappropriate dates\n  dplyr::filter(!is.na(date)) %>% \n  dplyr::mutate(\n    is_Pythagorean = (yy^2 == mm^2 + dd^2)\n  ) %>% \n  # keeping only Pythagorean days\n  dplyr::filter(is_Pythagorean) %>% \n  dplyr::select(date)\n\npythagorean_days\n\n# A tibble: 12 × 1\n   date      \n   <date>    \n 1 2005-03-04\n 2 2005-04-03\n 3 2010-06-08\n 4 2010-08-06\n 5 2013-05-12\n 6 2013-12-05\n 7 2015-09-12\n 8 2015-12-09\n 9 2017-08-15\n10 2020-12-16\n11 2025-07-24\n12 2026-10-24\n\n\nAs we can see, only 3 such days left in the 21st century, and one will be this year on December 16, 2020."
  },
  {
    "objectID": "posts/20200224/sampling-from-wischart.html",
    "href": "posts/20200224/sampling-from-wischart.html",
    "title": "Sampling from the Wischart distribution.",
    "section": "",
    "text": "The Wishart distribution is a family of probability distributions defined over symmetric, nonnegative-definite matrix-valued random variables (“random matrices”). These distributions are of great importance in the estimation of covariance matrices in multivariate statistics.\nSuppose \\(\\boldsymbol{X}_{p\\times n} = \\left(\\boldsymbol{X}^{(1)}, \\ldots, \\boldsymbol{X}^{n}\\right)\\) is a \\(p\\times n\\) matrix, each column of which is independently drawn from a \\(p\\)-variate normal distribution with zero mean, i.e.\n\\[\n  \\boldsymbol{X}^{(j)} \\sim N(0, \\boldsymbol{V}), \\quad j = 1, \\ldots, n,\n\\tag{1}\\]\nwhere \\(\\boldsymbol{V}\\) is a \\(p\\times p\\) covariance matrix. Then a random matrix-valued variable\n\\[\n  \\boldsymbol{S} = \\boldsymbol{X}\\cdot \\boldsymbol{X}'\n\\tag{2}\\]\nfollows Wishart distribution \\(W_p(\\boldsymbol{V}, n)\\). It has the following numerical characteristics:\n\nExpected value: \\[\n\\text{E}\\left[\\boldsymbol{S}\\right] = n\\boldsymbol{V}.\n\\tag{3}\\]\nVariance: \\[\n\\text{Var}\\left[\\boldsymbol{S}\\right] = n\\left(\\boldsymbol{V}^2 + diag(\\boldsymbol{V})\\cdot diag(\\boldsymbol{V})'\\right), \\: diag(\\boldsymbol{V}) = \\left(\\boldsymbol{V}_{11}, \\ldots, \\boldsymbol{V}_{pp}\\right),\n\\tag{4}\\]\n\nwhere elements of \\(\\boldsymbol{V}^2\\) are obtained as squares of the corresponding elements of \\(\\boldsymbol{V}\\).\nWe are interested in sampling random matrices with the mean value equal to \\(\\boldsymbol{V}\\), i.e.\n\\[\n  \\boldsymbol{S}_{1}, \\boldsymbol{S}_{2}, \\ldots, \\sim W_p(\\boldsymbol{V}, 1).\n\\tag{5}\\]\nA sample-point from the distribution \\(W_p(\\boldsymbol{V}, 1)\\) can be drawn by doing the following steps:\n\nSample multi-variate (\\(p\\)-variate) random variable \\(\\boldsymbol{X}\\sim N(0, \\boldsymbol{V})\\):\n\nmake a Choletsky decomposition of \\(\\boldsymbol{V}\\), i.e. \\(\\boldsymbol{V} = \\boldsymbol{L}\\cdot \\boldsymbol{L}'.\\)\nsample multi-variate (\\(p\\)-variate) random variable \\(\\boldsymbol{Z}\\sim N(0, \\boldsymbol{I}).\\)\n\\(\\boldsymbol{X} = \\boldsymbol{L}\\cdot \\boldsymbol{Z}.\\)\n\n\\(\\boldsymbol{S} = \\boldsymbol{X}\\cdot \\boldsymbol{X}'.\\)"
  },
  {
    "objectID": "posts/20200224/sampling-from-wischart.html#r-code-to-perform-the-procedure",
    "href": "posts/20200224/sampling-from-wischart.html#r-code-to-perform-the-procedure",
    "title": "Sampling from the Wischart distribution.",
    "section": "R code to perform the procedure",
    "text": "R code to perform the procedure\n\n# loading pipe %>%\nlibrary(magrittr)\n\n# loading map_ function family\nlibrary(purrr)\n\n# function to sample a single observation from the Wishart distribution, \n# given parameters V, n\n\nr1wishart <- function(V, n = 1){\n  p <- nrow(V)\n\n  # Choletsky decomposition \n  L <- t(chol(V))\n  \n  X <- map(seq_len(n), ~ { \n    Z <- rnorm(p)\n    L%*%Z\n  }) %>%\n  unlist() %>%\n  matrix(ncol = n, byrow = FALSE)\n  X%*%t(X) \n}\n\n# function to sample nsmp observations from the Wishart distribution, \n# given parameters V, n\n\nsample_wishart <- function(nsmp, V, n = 1){\n  map(seq_len(nsmp), ~ r1wishart(V, n)) \n}"
  },
  {
    "objectID": "posts/20200224/sampling-from-wischart.html#test-example",
    "href": "posts/20200224/sampling-from-wischart.html#test-example",
    "title": "Sampling from the Wischart distribution.",
    "section": "Test example",
    "text": "Test example\n\n# variances\nomega <- c(1, 2, 3)\n\n# correlation matrix\nrho <- rbind(\n  c(1, 0.2, 0.7),\n  c(0.2, 1, 0.45),\n  c(0.7, 0.45, 1) \n)\n\n# covariance matrix\nV <- rho*sqrt(cbind(omega)%*%rbind(omega)) \n\n# number of sample points\nnsmp <- 10000\n\n# sampling\nW <- sample_wishart(nsmp, V)\n\n# calculating the sample mean. We expect that it is approximately equal to V\nsample_mean <- reduce(W, `+`)/nsmp \n\n# printing out the sample mean\nprint(sample_mean) \n\n          [,1]      [,2]     [,3]\n[1,] 0.9942158 0.3034728 1.213229\n[2,] 0.3034728 2.0452006 1.148126\n[3,] 1.2132293 1.1481260 3.000992\n\n# printing out the true mean\nprint(V) \n\n          [,1]      [,2]     [,3]\n[1,] 1.0000000 0.2828427 1.212436\n[2,] 0.2828427 2.0000000 1.102270\n[3,] 1.2124356 1.1022704 3.000000\n\n# calculating the sample variance. \n# We expect that it is approximately equal to V^2 + diag(V)%*%t(diag(V))\n\nsample_var <- map(W, ~ .^2) %>% \n  reduce(`+`) %>% \n  '/'(nsmp) %>% \n  '-'(sample_mean^2) \n\n# printing out the result\nprint(round(nsmp/(nsmp-1)*sample_var, 2)) \n\n     [,1] [,2]  [,3]\n[1,] 1.88 2.10  4.25\n[2,] 2.10 8.10  7.21\n[3,] 4.25 7.21 17.73\n\n# printing out the true variance\nprint(V^2 + cbind(diag(V))%*%rbind(diag(V)))\n\n     [,1]  [,2]   [,3]\n[1,] 2.00 2.080  4.470\n[2,] 2.08 8.000  7.215\n[3,] 4.47 7.215 18.000"
  },
  {
    "objectID": "posts/20220824/two-interesting-nth-derivatives.html",
    "href": "posts/20220824/two-interesting-nth-derivatives.html",
    "title": "Calculation of the \\(n^\\text{th}\\) derivatives of \\(e^{ax}\\cos(bx)\\) and \\(e^{ax}\\sin(bx)\\)",
    "section": "",
    "text": "Given functions \\(g_1(x) = e^{ax}\\cos(bx)\\) and \\(g_2(x) = e^{ax}\\sin(bx)\\), calculate \\(\\frac{d^n}{dx^n}g_1(x)\\) and \\(\\frac{d^n}{dx^n}g_2(x)\\), \\(n = 1, 2, \\ldots\\)."
  },
  {
    "objectID": "posts/20220824/two-interesting-nth-derivatives.html#complex-analysis-approach",
    "href": "posts/20220824/two-interesting-nth-derivatives.html#complex-analysis-approach",
    "title": "Calculation of the \\(n^\\text{th}\\) derivatives of \\(e^{ax}\\cos(bx)\\) and \\(e^{ax}\\sin(bx)\\)",
    "section": "Complex analysis approach",
    "text": "Complex analysis approach\nLet us consider a function \\(f(x) = e^{(a+ib)x}\\):\n\\[\n\\begin{aligned}\nf(x) &= e^{ax}e^{ibx} = e^{ax}\\left(\\cos(bx) + i\\sin(bx)\\right) = \\\\\n&= e^{ax}\\cos(bx) + ie^{ax}\\sin(bx) = \\\\\n&= g_1(x) + ig_2(x) \\Rightarrow \\\\\n\\frac{d^n}{dx^n}f(x) &= \\frac{d^n}{dx^n}g_1(x) +i\\frac{d^n}{dx^n}g_2(x) \\Leftrightarrow \\\\ \\\\\n\\frac{d^n}{dx^n}g_1(x) &= \\mathfrak{Re}\\left(\\frac{d^n}{dx^n}f(x)\\right), \\\\\n\\frac{d^n}{dx^n}g_2(x) &= \\mathfrak{Im}\\left(\\frac{d^n}{dx^n}f(x)\\right).\n\\end{aligned}\n\\]\nHere, \\(\\mathfrak{Re}(\\cdot)\\) and \\(\\mathfrak{Im}(\\cdot)\\) are the real and imaginary parts of a complex number.\nThen, the \\(n^\\text{th}\\) derivative of \\(f(x)\\) is given by\n\\[\n\\begin{aligned}\n\\frac{d^n}{dx^n}f(x) &= (a+ib)^ne^{(a+ib)x} = \\\\\n&= \\left[\n\\begin{aligned}\na+ib &= re^{i\\theta}, \\text{ where} \\\\\nr &= \\sqrt{a^2+b^2} \\\\\n\\theta &= \\arg(a+ib) \\\\\n&\\Downarrow \\\\\n(a+ib)^n &= r^ne^{in\\theta}\n\\end{aligned}\n\\right] = \\\\ \\\\\n&= r^ne^{in\\theta}e^{(a+ib)x} = \\\\\n&= r^ne^{ax}\\left(\\cos(n\\theta)+i\\sin(n\\theta)\\right)\\left(\\cos(bx)+i\\sin(bx)\\right) = \\\\\n& \\begin{aligned}=r^ne^{ax}&\\left((\\underbrace{\\cos(bx)\\cos(n\\theta)-\\sin(bx)\\sin(n\\theta)}_{\\cos(bx+n\\theta)})\\right. + \\\\\n&+i\\left.(\\underbrace{\\sin(bx)\\cos(n\\theta)+\\cos(bx)\\sin(n\\theta)}_{\\sin(bx+n\\theta)})\\right)=\\end{aligned} \\\\\n&= r^ne^{ax}\\cos(bx + n\\theta) + ir^ne^{ax}\\sin(bx + n\\theta) \\Rightarrow \\\\ \\\\\n\\frac{d^n}{dx^n}g_1(x) &= \\frac{d^n}{dx^n}\\left(e^{ax}\\cos(bx)\\right) = r^ne^{ax}\\cos(bx + n\\theta), \\\\\n\\frac{d^n}{dx^n}g_2(x) &= \\frac{d^n}{dx^n}\\left(e^{ax}\\sin(bx)\\right) = r^ne^{ax}\\sin(bx + n\\theta), \\\\\n\\text{where } r &= \\sqrt{a^2+b^2},\\: \\theta = \\arg(a+ib). \\\\\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "posts/20220824/two-interesting-nth-derivatives.html#linear-algebra-approach",
    "href": "posts/20220824/two-interesting-nth-derivatives.html#linear-algebra-approach",
    "title": "Calculation of the \\(n^\\text{th}\\) derivatives of \\(e^{ax}\\cos(bx)\\) and \\(e^{ax}\\sin(bx)\\)",
    "section": "Linear algebra approach",
    "text": "Linear algebra approach\nLet us consider another approach that, from my point of view, is less efficient but is quite interesting.\nLet us consider a set of all linear combinations of functions \\(g_1(x)\\) and \\(g_2(x)\\):\n\\[\nS = \\text{span}\\left(g_1(x), g_2(x)\\right) = \\left\\{f(x) = c_1g_1(x) + c_2g_2(x)|c_1, c_2 \\in \\mathbb{R}\\right\\}.\n\\] Then, one may consider an operator \\(T: S \\rightarrow \\mathbb{R}^2\\) such as\n\\[\n\\forall f(x)=c_1g_1(x) + c_2g_2(x) \\in S,\\: T\\left[f(x)\\right] = \\begin{pmatrix}c_1 \\\\ c_2\\end{pmatrix} \\Leftrightarrow T^{-1}\\left[\\begin{pmatrix}c_1 \\\\ c_2\\end{pmatrix}\\right] = c_1g_1(x) + c_2g_2(x) = f(x).\n\\]\nNow, let us differentiate a function \\(f(x)\\in S\\):\n\\[\n\\begin{aligned}\n\\frac{d}{dx}f(x) &= \\frac{d}{dx}\\left(c_1g_1(x) + c_2g_2(x)\\right) = \\\\\n&= c_1\\frac{d}{dx}g_1(x) + c_2\\frac{d}{dx}g_2(x) = \\\\\n&= c_1\\frac{d}{dx}\\left(e^{ax}\\cos(bx)\\right) + c_2\\frac{d}{dx}\\left(e^{ax}\\sin(bx)\\right) = \\\\\n&= c_1\\left(ae^{ax}\\cos(bx) - be^{ax}\\sin(bx)\\right) + c_2\\left(ae^{ax}\\sin(bx) + be^{ax}\\cos(bx)\\right) = \\\\\n&= \\underbrace{(ac_1 + bc_2)}_{\\widetilde{c}_1}e^{ax}\\cos(bx) + \\underbrace{(-bc_1 + ac_2)}_{\\widetilde{c}_2}e^{ax}\\sin(bx) = \\\\\n&= \\widetilde{c}_1e^{ax}\\cos(bx) + \\widetilde{c}_2e^{ax}\\sin(bx) = \\\\\n&= \\widetilde{c}_1g_1(x) + \\widetilde{c}_2g_2(x) \\in S.\n\\end{aligned}\n\\]\nThe latter means that the differentiation operator \\(\\frac{d}{dx}\\) acts from \\(S\\) to \\(S\\), \\(\\frac{d}{dx}: S\\rightarrow S\\), and can be represented as\n\\[\n\\frac{d}{dx} = T^{-1}DT, \\text{ where } D\\left[\\begin{pmatrix}c_1 \\\\ c_2\\end{pmatrix}\\right] =\n\\begin{pmatrix} a & b \\\\ -b & a\\end{pmatrix}\\begin{pmatrix}c_1 \\\\ c_2\\end{pmatrix}.\n\\]\nThus, for \\(f(x) = c_1g_1(x) + c_2g_2(x) \\in S\\),\n\\[\n\\begin{aligned}\n\\frac{d}{dx}f(x) &= \\left(T^{-1}DT\\right)[f(x)] = T^{-1}\\left[D\\left[T\\left[f(x)\\right]\\right]\\right] = \\\\\n&= T^{-1}\\left[D\\left[\\begin{pmatrix}c_1 \\\\ c_2\\end{pmatrix}\\right]\\right] = \\\\\n&= T^{-1}\\left[\\begin{pmatrix} a & b \\\\ -b & a\\end{pmatrix}\\begin{pmatrix} c_1 \\\\ c_2\\end{pmatrix}\\right] = \\\\\n&= T^{-1}\\left[\\begin{pmatrix} ac_1 + bc_2 \\\\ -bc_1 + ac_2\\end{pmatrix}\\right] = T^{-1}\\left[\\begin{pmatrix} \\widetilde{c}_1\\\\ \\widetilde{c}_2\\end{pmatrix}\\right] = \\\\\n&= \\widetilde{c}_1g_1(x) + \\widetilde{c}_2g_2(x).\n\\end{aligned}\n\\]\nThen,\n\\[\n\\begin{aligned}\n\\frac{d^n}{dx^n}\\left[\\cdot\\right] &= \\underbrace{\\frac{d}{dx}\\frac{d}{dx}\\ldots\\frac{d}{dx}\\left[\\cdot\\right]}_{n\\text{ times}} = \\\\\n&= \\underbrace{(T^{-1}D\\overbrace{T)(T^{-1}}^{= I}DT)\\ldots (T^{-1}D \\overbrace{T)(T^{-1}}^{= I}DT) }_{n\\text{ times}} = \\left[I \\text{ is an identity operator}\\right] = \\\\\n&= T^{-1}\\underbrace{\\left(D\\ldots D\\right)}_{n\\text{ times}}T = \\\\\n&= T^{-1}D^nT,\n\\end{aligned}\n\\]\nwhere operator \\(D^n\\) is represented by \\(\\mathbf{D}^n\\), the \\(n^\\text{th}\\) power of matrix \\(\\mathbf{D} = \\begin{pmatrix} a & b \\\\ -b & a \\end{pmatrix}\\).\n\nCalculating \\(\\mathbf{D}^n\\)\nIf \\(\\mathbf{V} = \\left(\\mathbf{v}_1\\: \\mathbf{v}_2\\right)\\) is a matrix of eigenvectors of matrix \\(\\mathbf{D}\\), and \\(\\mathbf{\\Lambda} = \\begin{pmatrix}\\lambda_1 & 0 \\\\ 0 & \\lambda_2 \\end{pmatrix}\\) is a matrix of eigenvalues of matrix \\(\\mathbf{D}\\), then\n\\[\n\\mathbf{D} = \\mathbf{V}\\mathbf{\\Lambda}\\mathbf{V}^{-1}, \\text{ and } \\mathbf{D}^n = \\mathbf{V}\\mathbf{\\Lambda}^n\\mathbf{V}^{-1}.\n\\]\nLet us calculate eigenvalues \\(\\lambda_1\\), \\(\\lambda_2\\), and eigenvectors \\(\\mathbf{v}_1\\), \\(\\mathbf{v}_2\\).\n\\(\\lambda_1\\), \\(\\lambda_2\\) are solutions of the equation\n\\[\n\\det(\\mathbf{D}-\\lambda\\mathbf{I}) = 0 \\Leftrightarrow \\det\\begin{pmatrix}a-\\lambda & b \\\\ -b & a-\\lambda\\end{pmatrix} = (a-\\lambda)^2+b^2=0 \\Leftrightarrow\n\\left\\{\n\\begin{aligned}\n\\lambda_1 &= a+ib \\\\\n\\lambda_2 &= a-ib\n\\end{aligned}\n\\right.\n\\] Now, let us calculate eigenvectors:\n\\[\n\\begin{aligned}\n\\lambda_1 &= a+ib, \\: \\mathbf{v}_1 = \\begin{pmatrix}v_{11} \\\\ v_{12}\\end{pmatrix}, \\text{ and }\\mathbf{D}\\mathbf{v}_1 = \\lambda_1\\mathbf{v}_1 \\Leftrightarrow \\\\\n&\\left.\n\\begin{aligned}\nav_{11} + bv_{12} &= (a+ib)v_{11} \\\\\n-bv_{11} + av_{12} &= (a+ib)v_{12}\n\\end{aligned}\n\\right| \\Rightarrow \\mathbf{v}_1=\\begin{pmatrix}1 \\\\ i\\end{pmatrix}; \\\\ \\\\\n\\lambda_2 &= a-ib, \\: \\mathbf{v}_2 = \\begin{pmatrix}v_{21} \\\\ v_{22}\\end{pmatrix}, \\text{ and }\\mathbf{D}\\mathbf{v}_2 = \\lambda_2\\mathbf{v}_2 \\Leftrightarrow \\\\\n&\\left.\n\\begin{aligned}\nav_{21} + bv_{22} &= (a-ib)v_{21} \\\\\n-bv_{21} + av_{22} &= (a-ib)v_{22}\n\\end{aligned}\n\\right| \\Rightarrow \\mathbf{v}_2=\\begin{pmatrix}i \\\\ 1\\end{pmatrix}. \\\\ \\\\\n\\end{aligned}\n\\] Then,\n\\[\n\\mathbf{V} = \\begin{pmatrix} 1 & i \\\\ i & 1\\end{pmatrix}, \\:\n\\mathbf{\\Lambda} = \\begin{pmatrix} a+ib & 0 \\\\ 0 & a-ib\\end{pmatrix}, \\:\n\\mathbf{V}^{-1} = \\begin{pmatrix} 0.5 & -0.5i \\\\ -0.5i & 0.5\\end{pmatrix} \\Rightarrow\n\\]\n\\[\n\\begin{aligned}\n\\mathbf{D}^n = \\mathbf{V}\\mathbf{\\Lambda}^n\\mathbf{V}^{-1} &=\n\\begin{pmatrix} 1 & i \\\\ i & 1\\end{pmatrix}\n\\begin{pmatrix} (a+ib)^n & 0 \\\\ 0 & (a-ib)^n\\end{pmatrix}\n\\begin{pmatrix} 0.5 & -0.5i \\\\ -0.5i & 0.5\\end{pmatrix} = \\\\\n&= \\begin{pmatrix} 1 & i \\\\ i & 1\\end{pmatrix}\n\\begin{pmatrix} 0.5(a+ib)^n & -0.5i(a+ib)^n \\\\ -0.5i(a-ib)^n & 0.5(a-ib)^n\\end{pmatrix} = \\\\\n&= \\begin{pmatrix}\n   \\frac{(a+ib)^n + (a-ib)^n}{2} & \\frac{(a+ib)^n-(a-ib)^n}{2i} \\\\\n   -\\frac{(a+ib)^n - (a-ib)^n}{2i}& \\frac{(a+ib)^n+(a-ib)^n}{2}\n\\end{pmatrix} = \\\\\n&=\\left[\n\\begin{aligned}\n&\\text{notice that for }r = \\sqrt{a^2+b^2} \\text{ and }\\theta = \\arg(a+ib),\\\\\n&\\left.\\begin{aligned}\n(a+ib)^n &= (re^{i\\theta})^n = r^ne^{in\\theta} \\\\\n(a-ib)^n &= (re^{-i\\theta})^n = r^ne^{-in\\theta}\n\\end{aligned}\n\\right| \\Rightarrow \\\\ \\\\\n&\\frac{(a+ib)^n + (a-ib)^n}{2} = r^n\\frac{e^{in\\theta}+e^{-in\\theta}}{2} = r^n\\cos(n\\theta) \\\\\n&\\frac{(a+ib)^n - (a-ib)^n}{2i} = r^n\\frac{e^{in\\theta}-e^{-in\\theta}}{2i} = r^n\\sin(n\\theta) \\\\\n\\end{aligned}\n\\right] = \\\\\n&= \\begin{pmatrix} r^n\\cos(n\\theta) & r^n\\sin(n\\theta) \\\\ -r^n\\sin(n\\theta) & r^n\\cos(n\\theta)\\end{pmatrix}.\n\\end{aligned}\n\\]\n\n\nCalculating the \\(n^\\text{th}\\) derivative\nNow, we can calculate the derivative of \\(f(x) = c_1g_1(x) + c_2g_2(x) \\in S\\):\n\\[\n\\begin{aligned}\n\\frac{d^n}{dx^n}f(x) &= \\frac{d^n}{dx^n}\\left(c_1g_1(x)+c_2g_2(x)\\right) = c_1\\frac{d^n}{dx^n}g_1(x) + c_2\\frac{d^n}{dx^n}g_2(x) = \\\\\n&= T^{-1}D^nT\\left[c_1g_1(x)+c_2g_2(x)\\right] = \\\\\n&= T^{-1}\\left[ D^n\\left[\\begin{pmatrix} c_1 \\\\ c_2 \\end{pmatrix}\\right]\\right] = \\\\\n&= T^{-1}\\left[\\mathbf{D}^n\\begin{pmatrix} c_1 \\\\ c_2 \\end{pmatrix}\\right] = \\\\\n&= T^{-1}\\left[\\begin{pmatrix}r^n\\cos(n\\theta) & r^n\\sin(n\\theta) \\\\ -r^n\\sin(n\\theta) & r^n\\cos(n\\theta)\\end{pmatrix}\\begin{pmatrix} c_1 \\\\ c_2 \\end{pmatrix}\\right] = \\\\\n&= T^{-1}\\left[\\begin{pmatrix}c_1r^n\\cos(n\\theta) + c_2r^n\\sin(n\\theta) \\\\ \\\\ -c_1r^n\\sin(n\\theta) + c_2r^n\\cos(n\\theta)\\end{pmatrix}\\right] = \\\\ \\\\\n&= \\left(c_1r^n\\cos(n\\theta) + c_2r^n\\sin(n\\theta)\\right)e^{ax}\\cos(bx) + \\\\\n&+ \\left(-c_1r^n\\sin(n\\theta) + c_2r^n\\cos(n\\theta)\\right)e^{ax}\\sin(bx) = \\\\ \\\\\n&= c_1r^ne^{ax}\\underbrace{\\left(\\cos(bx)\\cos(n\\theta)-\\sin(bx)\\sin(n\\theta)\\right)}_{\\cos(bx+n\\theta)}+ \\\\\n&+ c_2r^ne^{ax}\\underbrace{\\left(\\sin(bx)\\cos(n\\theta)+\\cos(bx)\\sin(n\\theta)\\right)}_{\\sin(bx+n\\theta)} = \\\\\n&= c_1r^ne^{ax}\\cos(bx+n\\theta)+c_2r^ne^{ax}\\sin(bx+n\\theta) \\Rightarrow \\\\ \\\\\n\\frac{d^n}{dx^n}g_1(x) &= \\frac{d^n}{dx^n}\\left(e^{ax}\\cos(bx)\\right) = r^ne^{ax}\\cos(bx+n\\theta)\\text{ and } \\\\\n\\frac{d^n}{dx^n}g_2(x) &= \\frac{d^n}{dx^n}\\left(e^{ax}\\sin(bx)\\right) = r^ne^{ax}\\sin(bx+n\\theta), \\\\\n\\end{aligned}\n\\]\nwhich is the same result we obtained with complex analysis approach."
  },
  {
    "objectID": "posts/20190614/k-means-clustering-for-integration.html",
    "href": "posts/20190614/k-means-clustering-for-integration.html",
    "title": "k-means clustering approach to an integral evaluation",
    "section": "",
    "text": "Let us consider an integral\n\\[\n  I = \\int\\limits_{\\Omega}{f(x)\\pi(x)dx},\n\\tag{1}\\]\nwhere \\(f(x): \\Omega \\rightarrow \\mathbb{R}^n\\), \\(\\pi(x): \\Omega \\rightarrow \\mathbb{R}^n\\) is a probability density function, \\(x \\in \\Omega \\subseteq \\mathbb{R}^n\\).\nUsing Monte Carlo (MC) approach, the integral can be evaluates as\n\\[\n  I \\approx \\frac{1}{N}\\sum\\limits_{i = 1}^{N}{f(x_i)},\n\\tag{2}\\]\nwhere \\(\\{x_i\\}_{i = 1}^{N}\\) are sample points from the distribution \\(\\pi(x)\\). The lager the sample size \\(N\\), the more accurate the approximation. However, if \\(N\\) is sufficiently large, then the integral evaluation procedure becomes time-consuming."
  },
  {
    "objectID": "posts/20190614/k-means-clustering-for-integration.html#k-means-clustering-approach-to-integration",
    "href": "posts/20190614/k-means-clustering-for-integration.html#k-means-clustering-approach-to-integration",
    "title": "k-means clustering approach to an integral evaluation",
    "section": "K-means clustering approach to integration",
    "text": "K-means clustering approach to integration\nAn alternative approach can be applied, which is base on the k-means clustering. The algorithm is the following:\n\nSample \\(N\\) points from \\(\\pi(x)\\) distribution.\nApply k-means clustering to the sample obtained. An output of the clustering procedure is:\n\n\\(\\{X_j^{(0)}\\}_{j = 1}^K\\) – clusters’ centers.\n\\(\\{w_j\\}_{j = 1}^K\\) – proportions of sample points in \\(j\\text{the}\\) cluster.\n\nCalculate an approximated value of the integral as\n\n\\[\n  I \\approx \\sum\\limits_{j = 1}^{K}{w_jf(X_j^{(0)})}.\n\\tag{3}\\]"
  },
  {
    "objectID": "posts/20190614/k-means-clustering-for-integration.html#r-code-implementing-k-means-based-algorithm",
    "href": "posts/20190614/k-means-clustering-for-integration.html#r-code-implementing-k-means-based-algorithm",
    "title": "k-means clustering approach to an integral evaluation",
    "section": "R code implementing k-means-based algorithm",
    "text": "R code implementing k-means-based algorithm\n\n# \n# fcn -- function to be integrated.\n#   X -- array (N x n) of sample points from a corresponding \n#        distribution.\n#   K -- number of clusters.\nintegrate_kmeans <- function(fcn, X, K){\n  km <- kmeans(X,  K)         # we use R function kmeans\n  Xc <- km$centers            # centers of obtained clusters\n   w <- km$size/sum(km$size)  # proportions of sample points in clusters\n  \n  F <- purrr::map_dbl(seq_len(K), ~ fcn(Xc[., ])) \n  I <- sum(w*F)\n  \n  return(I)\n}"
  },
  {
    "objectID": "posts/20190614/k-means-clustering-for-integration.html#r-code-implementing-monte-carlo-based-algorithm",
    "href": "posts/20190614/k-means-clustering-for-integration.html#r-code-implementing-monte-carlo-based-algorithm",
    "title": "k-means clustering approach to an integral evaluation",
    "section": "R code implementing Monte Carlo-based algorithm",
    "text": "R code implementing Monte Carlo-based algorithm\n\n# fcn -- function to be integrated.\n#   X -- array (N x n) of sample points from a corresponding \n#        distribution.\nintegrate_mc <- function(fcn, X){\n  F <- purrr::map_dbl(seq_len(K), ~ fcn(X[., ])) \n  I <- mean(F)\n  \n  return(I)\n}"
  },
  {
    "objectID": "posts/20190614/k-means-clustering-for-integration.html#an-example",
    "href": "posts/20190614/k-means-clustering-for-integration.html#an-example",
    "title": "k-means clustering approach to an integral evaluation",
    "section": "An Example",
    "text": "An Example\nLet us evaluate a two-dimensional integral\n\\[\n  I = \\iint\\limits_{x_1^2 + x_2^2 \\leq 4}e^{-\\frac{(x_1^2+x_2^2)}{4}}dx_1dx_2,\n\\tag{4}\\]\nwhich can be rewritten as\n\\[\n\\begin{aligned}\n  I &= \\int\\limits_{-\\infty}^\\infty\\int\\limits_{-\\infty}^\\infty 4\\pi\\chi(x_1,     x_2)\\frac{1}{4\\pi}e^{-\\frac{(x_1^2+x_2^2)}{4}}dx_1dx_2 = \\\\\n  &= \\int\\limits_{-\\infty}^\\infty\\int\\limits_{-\\infty}^\\infty f(x_1, x_2)\\pi(x_1, x_2)dx_1dx_2,\n\\end{aligned}  \n\\tag{5}\\]\nwhere\n\\[\n  f(x_1, x_2) = 4\\pi\\chi(x_1, x_2)=\\left\\{\\begin{array}{rl} 4\\pi, & x_1^2+x_2^2 \\leq 4\\\\ 0, & otherwise\\end{array}\\right.,\n\\]\nand\n\\[\n  \\pi(x) = \\frac{1}{4\\pi}e^{-\\frac{(x_1^2+x_2^2)}{4}}\n\\]\nis a probability density function of a normal distribution \\(N\\left(\\mu = \\left(\\begin{array}{cc}0 \\\\ 0\\end{array}\\right), \\Sigma = \\left(\\begin{array}{cc}2 & 0 \\\\ 0 & 2\\end{array}\\right)\\right)\\).\n\nfcn <- function(x){\n  result <- 4*pi*ifelse(x[1]^2+x[2]^2 <= 4, 1, 0)\n  return(result)\n}\n\nn <- 2      # number of dimensions\nN <- 10000  # sample size from normal distribution\nK <- 8      # number of clusters for k-means clustering\n\n# sampling from N(mu, Sigma)\nmu <- c(0, 0)\nSigma <- diag(c(2, 2))\n\nset.seed(3141592)\nX <- MASS::mvrnorm(N, mu, Sigma)\n\n# integration by using MC\nmc_value <- integrate_mc(fcn, X)\n\n# integration by using k-means\nkmeans_value <- integrate_kmeans(fcn, X, K)\n\n# true value\ntrue_value <- 4*pi*(1-exp(-1))\n\ntibble::tibble(\n  true_value, mc_value, kmeans_value\n)                    \n\n# A tibble: 1 × 3\n  true_value mc_value kmeans_value\n       <dbl>    <dbl>        <dbl>\n1       7.94     7.85         7.73"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Yevgen Ryeznik",
    "section": "",
    "text": "My name is Yevgen Ryeznik (Ukrainian🇺🇦: Євген Рєзнік).\nI am a statistician working in the pharma industry, providing statistical support for the design and analysis of clinical trials.\nI obtained a PhD degree in applied mathematics and statistics at Uppsala University (Sweden) in 2019. The PhD project was a part of the Center for Interdisciplinary Mathematics (CIM) initiative. My supervisors were professor Warwick Tucker from the Department of Mathematics and professor Andrew Hooker from the Department of Pharmacy, Pharmacometrics Research Group.\nMy detailed CV is available here.\nI am interested in solving research problems in mathematics, (bio)statistics, pharmacometrics, and machine learning as well as in statistical programming and scientific computing, using R and Julia programming languages.\nHere, you can find information about several courses, that I teach/taught."
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications (2004-2023)",
    "section": "",
    "text": "Ryeznik, Y. (2019) “Optimal adaptive designs and adaptive randomization techniques for clinical trials.” Uppsala Dissertations in Mathematics,ISSN 1401-2049 113:94.  url  pdf"
  },
  {
    "objectID": "publications.html#peer-reviewed-publications",
    "href": "publications.html#peer-reviewed-publications",
    "title": "Publications (2004-2023)",
    "section": "Peer-reviewed publications",
    "text": "Peer-reviewed publications\n\n2022\nFei, Z., Ryeznik, Y., Sverdlov, O., Tan, Ch. W., Wong, W. K. (2022) “An overview of healthcare data analytics with applications to the COVID-19 pandemic.” IEEE Transactions on Big Data 8 (6):1463-1480. doi  arxiv\nSverdlov, O., Ryeznik, Y. (2022) “Accounting for patient engagement in randomized controlled trials evaluating digital cognitive behavioral therapies.” Applied Sciences 12 (10):4952. doi\n\n\n2021\nBerger, V.W., Bour, L. J., Carter, K., Chipman, J. J., Everett, C. C., Heussen, N, Hewitt, C, Hilgers, R. D., Luo, Y. A., Renteria, J., Ryeznik, Y., Sverdlov, O., Uschner, D. (2021) “A roadmap to using randomization in clinical trials.” BMC Medical Research Methodology 21 (168):1-24. doi  pdf  github\nRyeznik, Y., Sverdlov, O.,, Svensson, E. M., Montepiedra, G., Hooker, A. C., Wong, W. K. (2021) “Pharmacometrics meets statistics-A synergy for modern drug development.” CPT: Pharmacometrics & Systems Pharmacology 10 (10):1134-1149. doi  pdf\nSverdlov, O., Ryeznik, Y., Wong, W. K. (2021) “Opportunity for efficiency in clinical development: An overview of adaptive clinical trial designs and innovative machine learning tools, with examples from the cardiovascular field.” Contemporary Clinical Trials 105:106397. doi\nTyzhnenko, A. G., Ryeznik, Y. (2021) “Practical treatment of the multicollinearity: The optimal ridge method and the modified OLS.” Problems of Economy 1 (47):155-168. doi  pdf\n\n\n2020\nSverdlov, O., Ryeznik, Y., Wong, W. K. (2020) “On optimal designs for clinical trials: An updated review.” Journal of Statistical Theory and Practice 14 (10):1-29. doi\n\n\n2019\nTyzhnenko, A. G., Ryeznik, Y. (2019) “Ordinary least squares: the adequacy of linear regression solutions under multicollinearity and without it.” Problems of Economy 39 (1):217-227. doi  pdf\nSverdlov, O., Ryeznik, Y. (2019) “Implementing unequal randomization in clinical trials with heterogeneous treatment costs.” Statistics in Medicine 38 (16):2905-2927. doi\n\n\n2018\nRyeznik, Y., Sverdlov, O., Hooker, A. C. (2018) “Implementing optimal designs for dose–response studies through adaptive randomization for a small population group.” AAPS Journal 20 (85). doi  pdf\nRyeznik, Y., Sverdlov, O. (2018) “A comparative study of restricted randomization procedures for multiarm trials with equal or unequal treatment allocation ratios.” Statistics in Medicine 37 (21):3056-3077. doi  github\nRyeznik, Y., Sverdlov, O., Hooker, A. C. (2018) “Adaptive optimal designs for dose-finding studies with time-to-event outcomes.” AAPS Journal 20 (24). doi  pdf\n\n\n2015\nRyeznik, Y., Sverdlov, O., WOng, W. K. (2015) “RARtool: A MatLab software package for designing response-adaptive randomized clinical trials with time-to-event outcomes.” Journal of Statistical Software 66 (1). doi  pdf\nSverdlov, O., Ryeznik, Y., Wu, S. (2015) “Exact Bayesian inference comparing binomial proportions, with application to proof-of-concept clinical trials.” Therapeutic Innovation & Regulatory Science 49 (1):163-174. doi\n\n\n2014\nSverdlov, O., Ryeznik, Y., Wong, W. K. (2014) “Efficient and ethical response-adaptive randomization designs for multi-arm clinical trials with Weibull time-to-event outcomes.” 24 (4):732-754. doi\nSverdlov, O., Wong, W. K., Ryeznik, Y. (2014) “Adaptive clinical trial designs for phase I cancer studies.” Statistics Surveys 8:2-44. doi  pdf\n\n\n2013\nSverdlov, O., Rosenberger, W. F., Ryeznik, Y. (2013) “Utility of covariate-adjusted response-adaptive randomization in survival trials.” Statistics in Biopharmaceutical Research 5 (1):38-53. doi\n\n\n2012\nSverdlov, O., Ryeznik, Y., Wong, W. K. (2012) “Doubly-adaptive biased coin designs for balancing competing objectives in time-to-event trials.” Statistics and Its Interface 5 (1):401-413. doi  pdf\n\n\n2010\nTyzhnenko, A. G., Ryeznik, Y. (2010) “Peculiarities of electromagnetic wave scattering from water surface causing an anomalous effect.” Telecommunications and Radio Engineering 69 (6):531-536. doi\n\n\n2008\nRyeznik, Y., Sukharevsky, O. I., Tyzhnenko, A. G. (2008) “Detection of the scatterer’s form and size with multyfrequency radiolocation.” The Bulletin of Kharkiv National University 834 (13):31-36 (in Russian).\nTyzhnenko, A. G., Ryeznik, Y. (2008) “Potentially unbreakable ciphering on a hybrid physical-mathematical level.” The Bulletin of Kharkiv National University 834 (13):108-114.  pdf\nTyzhnenko, A. G., Ryeznik, Y. (2008) “Identification and localization of reflected pulses under strong noise.” The Bulletin of Kharkiv National University 806 (12):70-75 (in Russian).\n\n\n2007\nTyzhnenko, A. G., Ryeznik, Y. (2007) “Scattering from sea surface leading spike effect.” Raditechnics 149:27-31 (in Russian).\nTyzhnenko, A. G., Ryeznik, Y. (2007) “Estimation of accuracy of scattering field for 2D screens calculated with Method of Moments (MoM) in \\(L_2\\)” The Bulletin of Kharkiv National University 756:65-70 (in Russian).\nTyzhnenko, A. G., Ryeznik, Y. (2007) “Estimates of accuracy and efficiency of a MoM algorithm in \\(L_2\\) for 2-D screens” Progress in Electromagnetics Research 71:295-316. doi  pdf\nTyzhnenko, A. G., Ryeznik, Y. (2007) “Galerkin method solution as a reference solution for MoM algorithms in \\(L_2\\) for \\(E\\)-screens.” Journal of Applied Elecromagnetism 9 (1):27-44.  url\n\n\n2005\nTyzhnenko, A. G., Ryeznik, Y. (2005) “Convergent Galerkin MoM solution for 2-D \\(H\\)-scattering from screens.” Electromagnetics 25 (4):329-341. doi\n\n\n2004\nTyzhnenko, A. G., Ryeznik, Y. (2004) “Convergent Galerkin MoM solution to the hypersingular \\(1^\\text{st}\\) kind integral equation for 2-D Neumann problem.” Journal of Applied Elecromagnetism 6 (2):37-51.  url"
  },
  {
    "objectID": "publications.html#books-book-chapters-and-tutorials",
    "href": "publications.html#books-book-chapters-and-tutorials",
    "title": "Publications (2004-2023)",
    "section": "Books, book chapters, and tutorials",
    "text": "Books, book chapters, and tutorials\n\n2022\nSverdlov, O., Ryeznik, Y., Leonov, S., Fedorov, V. (2022) “Statistical approaches in the development of digital therapeutics” in Digital Therapeutics: Strategic, Scientific, Developmental, and Regulatory Aspects, chapter 6, Chapman and Hall/CRC.  url\nSverdlov, O., Ryeznik, Y., Wong, W. K. (2022) “Imbalanced randomization (rationales for).” in Wiley StatsRef: Statistics Reference Online, Wiley Online Library. doi\n\n\n2021\nSverdlov, O., Ryeznik, Y. (2021) “Randomization, stratification, and outcome-adaptive allocation” in Handbook of Statistical Methods for Randomized Clinical Trials, chapter 11: 215-242, Chapman and Hall/CRC. doi abstract\n\n\n2018\nKryvokhyzha, D., Ryeznik, Y. (2018) “Hail tutorial in Scala for population genomics ETL” in SDS-2.2, Scalable Data Science., GitBook.  url"
  },
  {
    "objectID": "publications.html#conference-presentations",
    "href": "publications.html#conference-presentations",
    "title": "Publications (2004-2023)",
    "section": "Conference presentations",
    "text": "Conference presentations\n\n2018\n“Computation of the geometric mean and variance of the AUC using polynomial chaos.” (with A. C. Hooker). PAGE Meeting: Montreux, Switzerland, May 29-June 1, 2018. abstract\n“Treatment allocation adaptive randomization methods in clinical trials with few individuals may influence model parameter estimation.” (with O. Sverdlov and A. C. Hooker). PAGE Meeting: Montreux, Switzerland, May 29-June 1, 2018. abstract  pdf\n“Implementing optimal designs for dose-response studies through adaptive randomization for a small population group.” (with O. Sverdlov and A. C. Hooker). Design of Experiments: New Challenges: Marseille, France, April 30-May 4, 2018.  url\n\n\n2016\n“Adaptive dose finding for time-to-event outcomes with adaptive choice of patient number based on response rate.” (with O. Sverdlov and A. C. Hooker). PAGE Meeting: Lisboa, Portugal, June 7-10, 2016.  pdf\n“Adaptive optimal designs for dose-finding studies with time-to-event outcomes on continuous dose space.” (with O. Sverdlov and A. C. Hooker). SIAM Conference on Uncertainty Quantification: Lausanne, Switzerland, April 5-8, 2016. abstract\n\n\n2015\n“Adaptive designs for dose finding clinical trials with time-to-event outcomes.” (with O. Sverdlov and A. C. Hooker). PAGE Meeting: Hersonissos, Crete, Greece, June 2-5, 2015.  pdf\n\n\n2012\n“Development of software for simulation and implementation of response-adaptive time-to-event trials.” (with O. Sverdlov and W. K. Wong). Joint Statistical Meetings (JSM): San Diego CA, USA, July 28-August 2, 2012. abstract\n\n\n2011\n“Efficient and ethical adaptive randomization designs for multi-armed clinical trials with Weibull time-to-event outcomes.” (with O. Sverdlov and W. K. Wong). The 20th Applied Statistics Symposium, International Chinese Statistical Association (ICSA): New York, USA, June 26-29, 2011. abstract\n\n\n2010\n“Covariate-adjusted response-adaptive randomization designs for phase III survival trials.” (with O. Sverdlov). XVII Biostatistics Applied Symposium (BASS): Hilton Head, SC, USA, 2010.  pdf"
  },
  {
    "objectID": "publications.html#conference-proceedings",
    "href": "publications.html#conference-proceedings",
    "title": "Publications (2004-2023)",
    "section": "Conference proceedings",
    "text": "Conference proceedings\n\n2007\nTyzhnenko, A. G., Ryeznik, Y. (2007) “Scattering amplitude error analysis for the MoM schemes in \\(L_2\\) commonly used for solving a 2-D scattering from screens.” The Sixth International Kharkov Symposium on Millimeter and Submillimeter Waves, MSMW-07: 319-321. Kharkiv, Ukraine. doi\n\n\n2006\nTyzhnenko, A. G., Ryeznik, Y. (2006) “A new insight into a possible spike mechanism.” International Conference on Mathematical Methods in Electromagnetics Theory, MMET-06: 170-172. Kharkiv, Ukraine. doi\n\n\n2005\nTyzhnenko, A. G., Ryeznik, Y. (2005) “A unique solution for \\(H\\)-scattering from 2-D roughness on a PEC plane.” 18th International Conference on Applied Electromagnetic, ICECom-05: 1-4. Dubrovnik, Croatia. doi"
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "Yevgen Ryeznik",
    "section": "",
    "text": "Mathematician/statistician with a strong interest in applications of mathematical and statistical methods (optimization, numerical methods, Monte Carlo simulations) to research problems in (bio)statistics, pharmacometrics, econometrics, machine learning, data science.\n15+ years of research experience in mathematical physics and biostatistics which is summarized by 25+ scientific publications in mathematical and statistical journals.\nPersonal web page: https://yevgenryeznik.github.io/personal-page"
  },
  {
    "objectID": "cv.html#fa-graduation-cap-education",
    "href": "cv.html#fa-graduation-cap-education",
    "title": "Yevgen Ryeznik",
    "section": " Education",
    "text": "Education\n\n\n Sep. 2014 – Apr. 2019  Uppsala, Sweden\n\n\nUppsala University, Department of Mathematics\nPh.D. in Applied Mathematics and Statistics\nThesis: “Optimal adaptive designs and adaptive randomization techniques for clinical trials.”\nCourses taken: Applied Mathematics, Analysis of Time Series, Uncertainty Quantification, Computational Python, Validated Numerics, Foundation of Machine Learning, Advanced Statistical Computing, Computer-Intensive Statistics, Introduction to Data Science, Foundation of Data Science.\n\n\n Sep. 1996 – Jun. 2001  Kharkiv, Ukraine\n\n\nKharkiv National University, Department of Mechanics and Mathematics\nSpecialist/M.Sc. in Applied Mathematics\nThesis: “Numerical analysis of a plane monochromatic wave scattering from a parabolic cylinder with a circular reflector.”\nCoursework included: Mathematical Analysis, Functional Analysis, Analytic and Differential Geometry, Linear and Abstract Algebra, Ordinary and Partial Differential Equations, Discrete Mathematics, Optimization, Probability, Mathematical Statistics, Random Processes, Game Theory, Numerical Analysis, Generalized Functions, Special Functions, Sobolev Spaces, Operator Theory, Diffraction Theory, Mechanics, Mathematical Physics."
  },
  {
    "objectID": "cv.html#fa-briefcase-work-experience",
    "href": "cv.html#fa-briefcase-work-experience",
    "title": "Yevgen Ryeznik",
    "section": " Work Experience",
    "text": "Work Experience\n\n\n Jan. 2023 – present  Gothenburg, Sweden\n\n\nThe Janssen Pharmaceutical Companies of Johnson & Johnson, Statistical Modeling and Methodology\nPrincipal Statistician\n\n\n Jun. 2020 – Dec. 2022  Gothenburg, Sweden\n\n\nAstraZeneca, Early Biometrics and Statistical Innovations\nSenior Statistician\n\nStatistical support for design and analysis of clinical trials.\nResearch on innovative statistical, pharmacometrics, and machine learning methods to improve clinical trials’ efficiency.\n\n\n\n Sep. 2019 – May 2020  Uppsala, Sweden\n\n\nUppsala University, Department of Pharmaceutical Biosciences\nResearcher in Pharmacometrics\n\nWorking on the FDA project “Evaluation and development of model-based bioequivalence analysis strategies.”\nDeveloping R package to perform the analysis.\n\n\n\n Sep. 2014 – Aug. 2019  Uppsala, Sweden\n\n\nUppsala University, Department of Mathematics\nPhD Student/Teacher of Mathematics & Statistics\n\nWorking on a Ph.D. thesis in biostatistics resulting in four scientific publications.\nTeaching (20%) mathematical and statistical courses for undergraduate, master, and Ph.D. students. Some courses were developed from scratch.\n\n\n\n Jan. 2013 – Aug. 2014  Kharkiv, Ukraine\n\n\nCS Ltd. (A part of Quartesian LLC)\nSAS Programmer/Clinical Data Analyst\n\nSAS programming and validating (QC) of SDTM and ADaM datasets, tables, listings, and figures for clinical study reports.\nDeveloping programming specifications according to CDISC standards.\nPerforming statistical analysis and modeling of clinical data.\n\n\n\n Sep. 2002 – Dec. 2012  Kharkiv, Ukraine\n\n\nKharkiv National University of Economics, Department of Mathematics\nTeaching and Research Assistant\n\nFull time teaching of various mathematical and statistical courses for undergraduate students.\nConducting research on mathematical physics and biostatistics problems.\n\n\n\nSep. 2001 – Aug. 2002  Kharkiv, Ukraine\n\n\nInformation Center of Trade\nSoftware Developer\n\nDeveloping software for cash registers.\nMaintenaning and supporting company’s databases."
  },
  {
    "objectID": "cv.html#fa-certificate-certificates",
    "href": "cv.html#fa-certificate-certificates",
    "title": "Yevgen Ryeznik",
    "section": " Certificates",
    "text": "Certificates\n\n\n2017  Uppsala, Sweden\n\n\nCenter for Interdisciplinary Mathematics, Uppsala University\nIntroduction & Fundamentals of Data Science\n\n\n2016  Uppsala, Sweden\n\n\nUppsala University\nAcademic Teacher Training\n\n\n2015  Uppsala, Sweden\n\n\nCenter for Interdisciplinary Mathematics, Uppsala University\nAdvanced Statistical Computing in R"
  },
  {
    "objectID": "cv.html#fa-cogs-skills",
    "href": "cv.html#fa-cogs-skills",
    "title": "Yevgen Ryeznik",
    "section": " Skills",
    "text": "Skills\n\n\n Quantitative\n\n\nAutomatic Differentiation, Bayesian Methods, Differential Equations, Experimental Designs, Interval Analysis, Machine Learning, Monte Carlo Methods, PK/PD Models, Probability & Statistics, Randomization, Regression, Survival Analysis, Uncertainty Quantification.\n\n\n Programming\n\n\nApache Spark, Julia, MatLab, Python, R, SAS, Scala, SQL.\n\n\n Tools\n\n\nDatabricks, Git, Jupyter Notebook, LaTeX, Quarto, RMarkdown."
  },
  {
    "objectID": "cv.html#fa-language-languages",
    "href": "cv.html#fa-language-languages",
    "title": "Yevgen Ryeznik",
    "section": " Languages",
    "text": "Languages\nEnglish (Fluent), Swedish (Beginner), Ukrainian (native), Russian (native)."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Yevgen Ryeznik",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nCalculation of the \\(n^\\text{th}\\) derivatives of \\(e^{ax}\\cos(bx)\\) and \\(e^{ax}\\sin(bx)\\)\n\n\n\n\n\n\n\nCalculus\n\n\nDerivative\n\n\nEigenvalues\n\n\nEigenvectors\n\n\n\n\nTwo approaches of calculating the \\(n^\\text{th}\\) derivatives: complex analysis and linear algebra techniques.\n\n\n\n\n\n\nAug 24, 2022\n\n\n3 min\n\n\n\n\n\n\n\n\nPythagorean theorem day\n\n\n\n\n\n\n\nR\n\n\nPythagorean theorem\n\n\n\n\nWhat is a “Pythagorean theorem day”? Calculating “Pythagorean theorem days” in the XXI century.\n\n\n\n\n\n\nFeb 29, 2020\n\n\n2 min\n\n\n\n\n\n\n\n\nSampling from the Wischart distribution.\n\n\n\n\n\n\n\nR\n\n\nStatistical simulations\n\n\nWishart distribution\n\n\n\n\nAn approach to sample from the Wischart distribtuion.\n\n\n\n\n\n\nFeb 24, 2020\n\n\n2 min\n\n\n\n\n\n\n\n\nk-means clustering approach to an integral evaluation\n\n\n\n\n\n\n\nk-means clustering\n\n\nIntegration\n\n\nMonte Carlo\n\n\nR\n\n\n\n\nk-means clustering approache to calculating multi-dimensional integrals as an alternative to Monte Carlo.\n\n\n\n\n\n\nJun 14, 2019\n\n\n2 min\n\n\n\n\n\n\nNo matching items"
  }
]